{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNuk6w3DcYz8dNJi51SObG7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsjadAfnan/AsjadAfnan/blob/main/LSTM_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYzBLyFq1q_T"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import requests\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Response  = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\n",
        "Response.text"
      ],
      "metadata": {
        "id": "U03knD_O2G_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Response.text.split('\\n')\n",
        "data[0]\n"
      ],
      "metadata": {
        "id": "euxS61UR4Gs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[253:]"
      ],
      "metadata": {
        "id": "eskFlEHH9oaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "gI4Qal_3-X93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data =\" \".join(data)"
      ],
      "metadata": {
        "id": "9JDk6WX5-l5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_txt (doc):\n",
        "  tokens = doc.split()\n",
        "  table = str.maketrans('','',string.punctuation)\n",
        "  tokens=[w.translate(table) for w in tokens]\n",
        "  tokens=[word for word in tokens if word.isalpha()]\n",
        "  tokens=[word.lower() for word in tokens]\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "mP7teNE3_s4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens =clean_txt(data)\n",
        "print(tokens[:50])"
      ],
      "metadata": {
        "id": "tt8wAe_qDxi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length=50+1\n",
        "lines=[]\n",
        "\n",
        "for i in range(length,len(tokens)):\n",
        "  seq= tokens[i-length:i]\n",
        "  line=' '.join(seq)\n",
        "  lines.append(line)\n",
        "  if i>200000:\n",
        "    break\n",
        "print (len(lines))"
      ],
      "metadata": {
        "id": "DHlQ23LOFFVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "id": "1-ju7AevGeAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "SWi2F-xCGjlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "aQaHTxj7IQeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences =np.array(sequences)\n",
        "X,Y = sequences[:,:-1] ,sequences[:,-1]\n",
        "X[0]"
      ],
      "metadata": {
        "id": "BMhJRDXxIzvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size= len(tokenizer.word_index)+1\n",
        "Y = to_categorical (Y,num_classes=vocab_size)\n"
      ],
      "metadata": {
        "id": "ZBNOH510I_ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=X.shape[1]\n",
        "seq_length"
      ],
      "metadata": {
        "id": "nrQLB-qvLY_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "model.add(LSTM(100,return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dense(vocab_size,activation='softmax'))"
      ],
      "metadata": {
        "id": "o0GPbnxrL0UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "feA5Izt2NF_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OXwhtOvteYG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y,batch_size=256,epochs=1)"
      ],
      "metadata": {
        "id": "YRvlOKtbe9YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,tokenizer,text_seq_length,seed_text,n_words):\n",
        "\n",
        "  for _  in range(n_words):\n",
        "    encoded=tokenizer.texts_to_sequence([seed_text])[0]\n",
        "    encoded=pad_sequences([encoded], maxlength=seq=length,truncating='pre')\n",
        "\n",
        "    Y_predict = model.predict_classes(encoded)\n",
        "\n",
        "    predicted_word=''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == Y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text+\" \"+Predicted_word\n",
        "    text.append(predicted_word)\n",
        "   return ''.join(text)"
      ],
      "metadata": {
        "id": "WWpdpdHyk6Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model,tokenizer,seq_length,seed_text,50)"
      ],
      "metadata": {
        "id": "5RhU3xEPqHFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}